{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c725cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv import *\n",
    "from fcn import *\n",
    "from client import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as torch\n",
    "import torch.nn.functional as F\n",
    "import crypten\n",
    "torch.set_printoptions(precision=16)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade7b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = 3\n",
    "input_channel = 10\n",
    "input_size = 128\n",
    "filter_channel = 10\n",
    "filter_size = 3\n",
    "act = 'ReLU'\n",
    "pooling_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1887de",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_list = []\n",
    "conv_list.append(Conv(input_batch, 3, input_size, filter_channel, filter_size, act, pooling_size))\n",
    "input_size = (int)(input_size / 2)\n",
    "\n",
    "while input_size > 1:\n",
    "    conv_list.append(Conv(input_batch, input_channel, input_size, filter_channel, filter_size, act, pooling_size))\n",
    "    input_size = (int)(input_size / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2d772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn_list = []\n",
    "output_size = 16\n",
    "\n",
    "fcn1 = FCN(input_batch, filter_channel, output_size)\n",
    "fcn_list.append(fcn1)\n",
    "\n",
    "while output_size > 2:\n",
    "    fcn_list.append(FCN(input_batch, output_size, (int)(output_size / 2)))\n",
    "    output_size = (int)(output_size / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d99989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    hidden_list = []\n",
    "    hidden_list.append(conv_list[0].forward(data))\n",
    "    \n",
    "    for i in range(1, len(conv_list)):\n",
    "        print(f'running convolution {i} ...')\n",
    "        temp = conv_list[i].forward(hidden_list[-1])\n",
    "        hidden_list.append(temp)\n",
    "        \n",
    "    hidden_list[-1] = hidden_list[-1].reshape(input_batch, 1, filter_channel)\n",
    "        \n",
    "    for fcn_layer in fcn_list:\n",
    "        print(f'running full connection ...')\n",
    "        temp = fcn_layer.forward(hidden_list[-1])\n",
    "        hidden_list.append(temp)\n",
    "        \n",
    "    return hidden_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c4153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/dhz/bci-data/ds003478-download/sub-010/eeg/sub-010_task-Rest_run-01_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/arithmetic.py:372: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  result.share = getattr(torch, op)(result.share, y, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/provider/tfp_provider.py:25: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c = getattr(torch, op)(a, b, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:82: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c._tensor += getattr(torch, op)(epsilon, b._tensor, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:83: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c._tensor += getattr(torch, op)(a._tensor, delta, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:84: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c += getattr(torch, op)(epsilon, delta, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running convolution 1 ...\n",
      "running convolution 2 ...\n",
      "running convolution 3 ...\n",
      "running convolution 4 ...\n",
      "running convolution 5 ...\n",
      "running convolution 6 ...\n",
      "running full connection ...\n",
      "running full connection ...\n",
      "running full connection ...\n",
      "running full connection ...\n"
     ]
    }
   ],
   "source": [
    "client = Client(10)\n",
    "data = client.get_data()\n",
    "\n",
    "prediction = train(data)\n",
    "\n",
    "loss = client.loss(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93603898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed64d1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = prediction.get_plain_text()\n",
    "pred.reshape(3, 2)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363d5056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.4804423692745441e+19, device='cuda:0', dtype=torch.float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbeb43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 2, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 2, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/arithmetic.py:243: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  result.share = torch.nn.functional.pad(\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 4, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 4, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 8, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 8, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 16, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 16, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 32, 32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 32, 32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 64, 64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 64, 64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 128, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 128, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "prev = client.loss_grad()\n",
    "\n",
    "for i in range(len(fcn_list)):\n",
    "    weight, prev = fcn_list[len(fcn_list) - 1 - i].backpropagation(prev, client, learning_rate)\n",
    "    \n",
    "prev = prev.reshape(input_batch, filter_channel, 1, 1)\n",
    "    \n",
    "for i in range(len(conv_list)):\n",
    "    weight, prev = conv_list[len(conv_list) - 1 - i].backpropagation(prev, client, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd75559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/__init__.py:64: RuntimeWarning: CrypTen is already initialized.\n",
      "  warnings.warn(\"CrypTen is already initialized.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/dhz/bci-data/ds003478-download/sub-010/eeg/sub-010_task-Rest_run-01_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/arithmetic.py:372: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  result.share = getattr(torch, op)(result.share, y, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/provider/tfp_provider.py:25: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c = getattr(torch, op)(a, b, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:82: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c._tensor += getattr(torch, op)(epsilon, b._tensor, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:83: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c._tensor += getattr(torch, op)(a._tensor, delta, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:84: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c += getattr(torch, op)(epsilon, delta, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running convolution 1 ...\n",
      "running convolution 2 ...\n",
      "running convolution 3 ...\n",
      "running convolution 4 ...\n",
      "running convolution 5 ...\n",
      "running convolution 6 ...\n",
      "running full connection ...\n",
      "running full connection ...\n",
      "running full connection ...\n",
      "running full connection ...\n"
     ]
    }
   ],
   "source": [
    "client = Client(10)\n",
    "data = client.get_data()\n",
    "\n",
    "prediction = train(data)\n",
    "\n",
    "loss = client.loss(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1b65c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1915001731937206e+19, device='cuda:0', dtype=torch.float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a5f4132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/__init__.py:64: RuntimeWarning: CrypTen is already initialized.\n",
      "  warnings.warn(\"CrypTen is already initialized.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/dhz/bci-data/ds003478-download/sub-010/eeg/sub-010_task-Rest_run-01_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/arithmetic.py:372: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  result.share = getattr(torch, op)(result.share, y, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/provider/tfp_provider.py:25: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c = getattr(torch, op)(a, b, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:82: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c._tensor += getattr(torch, op)(epsilon, b._tensor, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:83: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c._tensor += getattr(torch, op)(a._tensor, delta, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:84: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c += getattr(torch, op)(epsilon, delta, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running convolution 1 ...\n",
      "running convolution 2 ...\n",
      "running convolution 3 ...\n",
      "running convolution 4 ...\n",
      "running convolution 5 ...\n",
      "running convolution 6 ...\n",
      "running full connection ...\n",
      "running full connection ...\n",
      "running full connection ...\n",
      "running full connection ...\n"
     ]
    }
   ],
   "source": [
    "client = Client(10)\n",
    "data = client.get_data()\n",
    "\n",
    "prediction = train(data)\n",
    "\n",
    "loss = client.loss(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad4c59c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1915001731937206e+19, device='cuda:0', dtype=torch.float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c9b8a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 2, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 2, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/arithmetic.py:243: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  result.share = torch.nn.functional.pad(\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 4, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 4, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 8, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 8, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 16, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 16, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 32, 32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 32, 32]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 64, 64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 64, 64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:25: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 128, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dp_dr = torch.eq(p_plain.repeat_interleave(2, dim=2).repeat_interleave(2, dim=3), relu_plain, out=torch.tensor([1.,0.]).cuda())\n",
      "/home/dhz/experiment-BCI/client.py:28: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3, 10, 128, 128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  dr_dc = torch.eq(relu_plain, conv_plain, out=torch.tensor([1.,0.]).cuda())\n"
     ]
    }
   ],
   "source": [
    "pred = prediction.get_plain_text()\n",
    "pred.reshape(3, 2)\n",
    "pred.shape\n",
    "\n",
    "learning_rate = 0.01\n",
    "prev = client.loss_grad()\n",
    "\n",
    "for i in range(len(fcn_list)):\n",
    "    weight, prev = fcn_list[len(fcn_list) - 1 - i].backpropagation(prev, client, learning_rate)\n",
    "    \n",
    "prev = prev.reshape(input_batch, filter_channel, 1, 1)\n",
    "    \n",
    "for i in range(len(conv_list)):\n",
    "    weight, prev = conv_list[len(conv_list) - 1 - i].backpropagation(prev, client, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3c2e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/__init__.py:64: RuntimeWarning: CrypTen is already initialized.\n",
      "  warnings.warn(\"CrypTen is already initialized.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/dhz/bci-data/ds003478-download/sub-010/eeg/sub-010_task-Rest_run-01_eeg.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/arithmetic.py:372: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  result.share = getattr(torch, op)(result.share, y, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/provider/tfp_provider.py:25: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c = getattr(torch, op)(a, b, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:82: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c._tensor += getattr(torch, op)(epsilon, b._tensor, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:83: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c._tensor += getattr(torch, op)(a._tensor, delta, *args, **kwargs)\n",
      "/home/dhz/anaconda3/lib/python3.9/site-packages/crypten/mpc/primitives/beaver.py:84: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)\n",
      "  c += getattr(torch, op)(epsilon, delta, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running convolution 1 ...\n",
      "running convolution 2 ...\n",
      "running convolution 3 ...\n",
      "running convolution 4 ...\n",
      "running convolution 5 ...\n",
      "running convolution 6 ...\n",
      "running full connection ...\n",
      "running full connection ...\n",
      "running full connection ...\n",
      "running full connection ...\n"
     ]
    }
   ],
   "source": [
    "client = Client(10)\n",
    "data = client.get_data()\n",
    "\n",
    "prediction = train(data)\n",
    "\n",
    "loss = client.loss(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2b1475c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1401579260295512e+19, device='cuda:0', dtype=torch.float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65e5cc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPCTensor(\n",
       "\t_tensor=CUDALongTensor(tensor([[[  58436303036957,  526929959574734]],\n",
       "\n",
       "        [[ 237259700864000, -270505036971923]],\n",
       "\n",
       "        [[-149419957694254,   93122587598527]]], device='cuda:0'))\n",
       "\tplain_text=HIDDEN\n",
       "\tptype=ptype.arithmetic\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e661ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.9166720000000000e+08,  8.0403133440000000e+09]],\n",
       "\n",
       "        [[ 3.6202956800000000e+09, -4.1276423680000000e+09]],\n",
       "\n",
       "        [[-2.2800023040000000e+09,  1.4209379840000000e+09]]], device='cuda:0',\n",
       "       dtype=torch.float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.get_plain_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c682829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8aac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
